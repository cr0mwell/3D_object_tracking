The following functionality has been implemented:
* matchBoundingBoxes function was implemented in camFusion.cpp that loops through the keypoint matches in previous and current frames and calculates the number of correspondent keypoints in the boundig boxes.
Then it maps the bounding boxes that have the max number of matched keypoints.
* LiDAR TTC computation was implemented in computeTTCLidar function. First it filters the outliers using Z-score method (sequentially by the median and the mean with 3*std distance check). Next TTC is calculated using the closest keypoint to the object in the current and in the previous frames.
* clusterKptMatchesWithROI function associates a given bounding box with the keypoints it contains. It also filters the outliers using Z-score method (sequentially by the median and the mean with 3*std distance check).
* Camera-based TTC is calculated in the computeTTCCamera function. First the matching keypoints are collected from the current and the previous frames. Next the keypoints are filtered using z-score method applied to the x,y keypoint coordinates. After that a mean of the keypoint distance ratios is calculated to evaluate the TTC. These distances are filtered by the threshold 20.

Conclusion regarding LiDAR processing:
according to the tests, FPs were filtered quite well (using Z-score median/mean filtering), therefore they couldn't affect the calculation results. Checks with bird-view frames confirm that the distance was measured precisely for all of them. Huge frame-to-frame estimation changes are related to the invalid velocity model that has been chosen. Neither Ego vehicle nor preceding vehicle's speed is constant. These speed changes in short frame rate intervals cause TTC value spikes.

Conclusion regarding camera processing:
let's not forget that the camera is not good at all at distance estimation. As distance is the only variable in the formula, we shouldn't expect a good results in TTC estimation, especially when we chose an invalid velocity model (constant speed). Another factor that affects the accuracy is detector-descriptor combination that sometimes provides a lot FPs. After a several filtering procedures there are few valid matches left (5-20) which can't give a good TTC estimation performance. Not mentioning some detectors, like Harris or ORB that detect 200-500 points in general for the image which is usually not sufficient to track the preceding vehicle.
The results are not very good therefore. FAST/BRIEF, FAST/SIFT, AKAZE/AKAZE detector/descriptor pairs showed the best accuracy overall. Still the mean difference in TTC estimation with LiDAR is about 2,5 sec. We could enhance the accuracy if another velocity model was chosen.

Resulting TTC scores for each deteector/descriptor pairs can be found in TTC_spreadsheet.xls file.
